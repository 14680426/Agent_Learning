"""
è¯­ä¹‰æ–‡æœ¬åˆ†å—å™¨
"""

from typing import List, Optional
import numpy as np
import re
import os
from modelscope.hub.snapshot_download import snapshot_download
from langchain_community.embeddings import HuggingFaceEmbeddings
from RAG.tools.splits.base import TextSplitter


class SemanticTextSplitter(TextSplitter):
    """
    è¯­ä¹‰æ–‡æœ¬åˆ†å—å™¨
    ç»§æ‰¿è‡ª TextSplitter åŸºç±»
    """

    def __init__(self, 
                 embeddings=None,
                 similarity_threshold: float = 0.6):
        """
        åˆå§‹åŒ–è¯­ä¹‰æ–‡æœ¬åˆ†å—å™¨
        
        Args:
            embeddings: åµŒå…¥æ¨¡å‹ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½é»˜è®¤çš„æœ¬åœ°æ¨¡å‹
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„å¥å­å°†è¢«åˆ†åˆ°ä¸åŒå—ä¸­
        """
        if embeddings is None:
            # åˆ›å»ºæœ¬åœ°æ¨¡å‹ç›®å½•ï¼Œä»srcç›®å½•å¼€å§‹
            local_models_dir = os.path.join(os.path.dirname(__file__), "..", "..", "..", "Models")
            local_models_dir = os.path.abspath(local_models_dir)
            os.makedirs(local_models_dir, exist_ok=True)

            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æ¨¡å‹ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ä» ModelScope ä¸‹è½½
            model_id = "maidalun/bce-embedding-base_v1"

            # æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„
            local_model_path = os.path.join(local_models_dir, "maidalun", "bce-embedding-base_v1")

            # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨æˆ–è€…ç›®å½•ä¸ºç©ºï¼Œåˆ™ä» ModelScope ä¸‹è½½
            if not os.path.exists(local_model_path) or not os.listdir(local_model_path):
                print(f"ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ {model_id}ï¼Œæ­£åœ¨ä» ModelScope ä¸‹è½½...")
                # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(local_model_path), exist_ok=True)
                local_model_path = snapshot_download(model_id, local_dir=local_model_path)
                print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_path}")
            else:
                print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")

            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¯ç”¨ GPUï¼‰
            self.embeddings = HuggingFaceEmbeddings(
                model_name=local_model_path,
                model_kwargs={"device": "cuda"},               # ä½¿ç”¨ GPU åŠ é€Ÿ
                encode_kwargs={"normalize_embeddings": True}   # å½’ä¸€åŒ–ä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            )
        else:
            self.embeddings = embeddings
            
        self.similarity_threshold = similarity_threshold
        print(f"è¯­ä¹‰æ–‡æœ¬åˆ†å—å™¨å·²åˆå§‹åŒ–ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼ä¸º {similarity_threshold}")
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2
            
        Returns:
            float: ä½™å¼¦ç›¸ä¼¼åº¦å€¼
        """
        if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:
            return 0.0
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

    def _split_sentences(self, text: str) -> List[str]:
        """
        åˆ†å‰²å¥å­ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰
        
        Args:
            text: å¾…åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            List[str]: å¥å­åˆ—è¡¨
        """
        sentence_endings = r'[ã€‚ï¼ï¼Ÿï¼›\n]'
        sentences = re.split(sentence_endings, text)
        # æ›´ä¸¥æ ¼çš„è¿‡æ»¤æ¡ä»¶
        sentences = [s.strip() for s in sentences if s and s.strip()]
        # ç¡®ä¿å¥å­ä»¥æ ‡ç‚¹ç»“å°¾
        sentences = [s + 'ã€‚' for s in sentences if not s.endswith(('ã€‚', '!', '?', 'ï¼', 'ï¼Ÿ', 'ï¼›'))]
        return sentences

    def _get_sentence_embeddings(self, sentences: List[str]) -> List[np.ndarray]:
        """
        è·å–å¥å­åµŒå…¥
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            
        Returns:
            List[np.ndarray]: å¥å­åµŒå…¥å‘é‡åˆ—è¡¨
        """
        embeddings = []
        for sent in sentences:
            # å¢åŠ æ–‡æœ¬éªŒè¯
            if not sent or not isinstance(sent, str):
                print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆå¥å­: {repr(sent)}")
                # ä½¿ç”¨é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                embeddings.append(np.zeros(768))  # å‡è®¾768ç»´å‘é‡ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                continue
                
            # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦
            cleaned_sent = self._clean_text(sent)
            if not cleaned_sent:
                print(f"è­¦å‘Š: æ¸…ç†åå¥å­ä¸ºç©º: {repr(sent)}")
                embeddings.append(np.zeros(768))
                continue
                
            try:
                emb = self.embeddings.embed_query(cleaned_sent)
                embeddings.append(np.array(emb))
            except Exception as e:
                print(f"è­¦å‘Š: å¤„ç†å¥å­ '{sent[:50]}...' æ—¶å‡ºé”™: {e}")
                # å‡ºé”™æ—¶ä½¿ç”¨é›¶å‘é‡
                embeddings.append(np.zeros(768))
        return embeddings

    def _clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´tokenizeré”™è¯¯çš„å­—ç¬¦
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤æ§åˆ¶å­—ç¬¦å’Œå…¶ä»–å¯èƒ½å¯¼è‡´é—®é¢˜çš„å­—ç¬¦
        cleaned = ''.join(char for char in text if ord(char) >= 32 or char in '\n\t')
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned = ' '.join(cleaned.split())
        return cleaned

    def split_text(self, text: str) -> List[str]:
        """
        åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—
        
        Args:
            text: å¾…åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text or not isinstance(text, str):
            return [text] if text else [""]
        
        # 1. æŒ‰å¥å­åˆ†å‰²
        sentences = self._split_sentences(text)
        if len(sentences) <= 1:
            return [text]
        
        # 2. è®¡ç®—æ¯ä¸ªå¥å­çš„åµŒå…¥
        embeddings = self._get_sentence_embeddings(sentences)
        
        # 3. è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦
        similarities = []
        for i in range(len(embeddings) - 1):
            sim = self.cosine_similarity(embeddings[i], embeddings[i+1])
            similarities.append(sim)
        
        # 4. åœ¨ç›¸ä¼¼åº¦ä½çš„åœ°æ–¹åˆ‡åˆ†
        chunks = []
        current_chunk = [sentences[0]]
        
        for i, sim in enumerate(similarities):
            if sim < self.similarity_threshold:
                chunks.append(''.join(current_chunk))
                current_chunk = [sentences[i+1]]
            else:
                current_chunk.append(sentences[i+1])
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(''.join(current_chunk))
        
        return chunks

    def split_documents(self, documents) -> List:
        """
        å°†æ–‡æ¡£åˆ—è¡¨åˆ†å‰²æˆå¤šä¸ªæ–‡æ¡£å—
        
        Args:
            documents: å¾…åˆ†å‰²çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List: åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        # è¿™é‡Œä¸ºäº†ä¿æŒæ¥å£ä¸€è‡´æ€§ï¼Œä½†å®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å®ç°
        all_chunks = []
        for doc in documents:
            # ç¡®ä¿docæ˜¯ä¸€ä¸ªæ–‡æ¡£å¯¹è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²
            if isinstance(doc, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ–‡æ¡£å¯¹è±¡
                text = doc
                source_metadata = {}
            else:
                # å¦‚æœæ˜¯æ–‡æ¡£å¯¹è±¡ï¼Œæå–æ–‡æœ¬å†…å®¹
                text = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                source_metadata = getattr(doc, 'metadata', {})
            
            chunks = self.split_text(text)
            
            # ä¸ºæ¯ä¸ªå—åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            for i, chunk in enumerate(chunks):
                # åˆ›å»ºç®€å•çš„æ–‡æ¡£å¯¹è±¡ç»“æ„
                chunk_doc = type('Document', (), {
                    'page_content': chunk,
                    'metadata': source_metadata.copy()
                })()
                # æ·»åŠ å—ç´¢å¼•ä¿¡æ¯
                chunk_doc.metadata["chunk_index"] = i
                chunk_doc.metadata["total_chunks"] = len(chunks)
                # ç¡®ä¿åŒ…å«æºæ–‡ä»¶ä¿¡æ¯
                if "source" not in chunk_doc.metadata and "source" in source_metadata:
                    chunk_doc.metadata["source"] = source_metadata["source"]
                all_chunks.append(chunk_doc)
                
        return all_chunks


if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬
    sample_text = """
    äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
    è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
    äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ã€‚
    å¯ä»¥è®¾æƒ³ï¼Œæœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„"å®¹å™¨"ã€‚
    äººå·¥æ™ºèƒ½å¯ä»¥å¯¹äººçš„æ„è¯†ã€æ€ç»´çš„ä¿¡æ¯è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚
    äººå·¥æ™ºèƒ½ä¸æ˜¯äººçš„æ™ºèƒ½ï¼Œä½†èƒ½åƒäººé‚£æ ·æ€è€ƒã€ä¹Ÿå¯èƒ½è¶…è¿‡äººçš„æ™ºèƒ½ã€‚
    äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨æå¯ŒæŒ‘æˆ˜æ€§çš„ç§‘å­¦ã€‚
    ä»äº‹è¿™é¡¹å·¥ä½œçš„äººå¿…é¡»æ‡‚å¾—è®¡ç®—æœºçŸ¥è¯†ï¼Œå¿ƒç†å­¦å’Œå“²å­¦ã€‚
    äººå·¥æ™ºèƒ½æ˜¯åŒ…æ‹¬ååˆ†å¹¿æ³›çš„ç§‘å­¦ï¼Œå®ƒç”±ä¸åŒçš„é¢†åŸŸç»„æˆï¼Œ
    å¦‚æœºå™¨å­¦ä¹ ï¼Œè®¡ç®—æœºè§†è§‰ç­‰ç­‰ï¼Œæ€»çš„è¯´æ¥ï¼Œäººå·¥æ™ºèƒ½ç ”ç©¶çš„ä¸€ä¸ªä¸»è¦ç›®æ ‡æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿèƒœä»»ä¸€äº›é€šå¸¸éœ€è¦äººç±»æ™ºèƒ½æ‰èƒ½å®Œæˆçš„å¤æ‚å·¥ä½œã€‚
    """

    # å®ä¾‹åŒ–è¯­ä¹‰æ–‡æœ¬åˆ†å—å™¨
    splitter = SemanticTextSplitter(similarity_threshold=0.5)
    
    # è°ƒç”¨åˆ†å‰²æ–¹æ³•
    chunks = splitter.split_text(sample_text)
    
    # æ‰“å°ç»“æœ
    print(f"è¯­ä¹‰åˆ†å—ç»“æœ: {len(chunks)} å—")
    for i, chunk in enumerate(chunks):
        print(f"\nå— {i+1}:\n{chunk}")