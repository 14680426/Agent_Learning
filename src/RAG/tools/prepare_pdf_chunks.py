#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import hashlib
import datetime
from typing import List, Optional
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from modelscope.hub.snapshot_download import snapshot_download
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
import sys

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•(src/RAG/tools)
current_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–srcç›®å½•
src_dir = os.path.dirname(os.path.dirname(current_dir))
# å°†srcç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä¸­
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

from dotenv import load_dotenv
# åŠ è½½é¡¹ç›®æ ¹ç›®å½•ä¸­çš„.envæ–‡ä»¶
project_root = os.path.dirname(src_dir)
load_dotenv(dotenv_path=os.path.join(project_root, '.env'))


# åˆ›å»ºæœ¬åœ°æ¨¡å‹ç›®å½•
local_models_dir = "Models"
os.makedirs(local_models_dir, exist_ok=True)

# æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æ¨¡å‹ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ä» ModelScope ä¸‹è½½
model_id = "maidalun/bce-embedding-base_v1"

# æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„
local_model_path = os.path.join(local_models_dir, "maidalun", "bce-embedding-base_v1")

# å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨æˆ–è€…ç›®å½•ä¸ºç©ºï¼Œåˆ™ä» ModelScope ä¸‹è½½
if not os.path.exists(local_model_path) or not os.listdir(local_model_path):
    print(f"ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ {model_id}ï¼Œæ­£åœ¨ä» ModelScope ä¸‹è½½...")
    # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(local_model_path), exist_ok=True)
    local_model_path = snapshot_download(model_id, local_dir=local_model_path)
    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_path}")
else:
    print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¯ç”¨ GPUï¼‰
embeddings = HuggingFaceEmbeddings(
    model_name=local_model_path,
    model_kwargs={"device": "cuda"},               # ä½¿ç”¨ GPU åŠ é€Ÿ
    encode_kwargs={"normalize_embeddings": True}   # å½’ä¸€åŒ–ä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
)

def clean_text(text: str) -> str:
    """
    ä¼˜åŒ–çš„æ–‡æœ¬æ¸…ç†å‡½æ•°ï¼Œæ›´å¥½åœ°å¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
    ç§»é™¤æ— æ³•ç¼–ç ä¸º UTF-8 çš„éæ³•å­—ç¬¦ï¼Œå¹¶æ¸…ç†é¦–å°¾ç©ºç™½
    """
    # ç§»é™¤æ— æ³•ç¼–ç ä¸º UTF-8 çš„éæ³•å­—ç¬¦
    text = text.encode('utf-8', errors='ignore').decode('utf-8')
    
    # å®šä¹‰ä¸­æ–‡å­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
    chinese_char = r'[\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff]'
    
    # å®šä¹‰å„ç§ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬å…¨è§’ç©ºæ ¼ã€ä¸é—´æ–­ç©ºæ ¼ç­‰ï¼‰
    any_whitespace = r'[\s\u00A0\u2000-\u200F\u2028-\u202F\u3000]+'
    
    # åˆ é™¤ä¸­æ–‡å­—ç¬¦ä¹‹é—´çš„ç©ºç™½å­—ç¬¦
    pattern = f'({chinese_char}){any_whitespace}(?={chinese_char})'
    text = re.sub(pattern, r'\1', text)
    
    # å¤„ç†æ¢è¡Œç¬¦ï¼Œå°†å•ä¸ªæ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
    text = re.sub(r'(?<!\n)\n(?!\n)', ' ', text)
    
    # å°†å¤šä¸ªè¿ç»­æ¢è¡Œç¬¦åˆå¹¶ä¸ºä¸¤ä¸ªæ¢è¡Œç¬¦ï¼ˆè¡¨ç¤ºæ®µè½åˆ†éš”ï¼‰
    text = re.sub(r'\n{2,}', '\n\n', text)
    
    # åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
    text = re.sub(r' +', ' ', text)
    
    # å¤„ç†ç‰¹æ®Šå­—ç¬¦
    # åˆ é™¤é›¶å®½å­—ç¬¦
    text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', text)
    
    # æ¸…ç†é¦–å°¾ç©ºç™½
    return text.strip()

def get_text_splitter(split_type: str = "recursive", embeddings=None):
    """
    è·å–æ–‡æœ¬åˆ†å‰²å™¨
    
    Args:
        split_type: åˆ†å‰²ç±»å‹ï¼Œ"recursive" æˆ– "semantic"
        embeddings: åµŒå…¥æ¨¡å‹ï¼Œä»…åœ¨ä½¿ç”¨è¯­ä¹‰åˆ†å‰²æ—¶éœ€è¦
        
    Returns:
        TextSplitter: æ–‡æ¡£åˆ†å‰²å™¨å®ä¾‹
    """
    # å¯¼å…¥æˆ‘ä»¬æ–°åˆ›å»ºçš„åˆ†å—æ–¹æ³•
    from RAG.tools.splits import RecursiveTextSplitter, SemanticTextSplitter
    
    if split_type == "semantic":
        return SemanticTextSplitter(embeddings, similarity_threshold=0.75)
    else:
        return RecursiveTextSplitter(chunk_size=300, chunk_overlap=30)

def load_pdfs_from_directory(directory_path: str) -> List:
    """
    ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰PDFæ–‡ä»¶
    
    Args:
        directory_path: PDFæ–‡ä»¶ç›®å½•è·¯å¾„
        
    Returns:
        List: æ–‡æ¡£åˆ—è¡¨
    """
    documents = []
    # æ£€æŸ¥è·¯å¾„æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
    if os.path.isfile(directory_path) and directory_path.lower().endswith('.pdf'):
        # å¦‚æœæ˜¯å•ä¸ªPDFæ–‡ä»¶
        loader = PyPDFLoader(directory_path)
        documents.extend(loader.load())
    elif os.path.isdir(directory_path):
        # å¦‚æœæ˜¯ç›®å½•ï¼ŒåŠ è½½ç›®å½•ä¸­æ‰€æœ‰PDFæ–‡ä»¶
        for file_name in os.listdir(directory_path):
            if file_name.lower().endswith('.pdf'):
                file_path = os.path.join(directory_path, file_name)
                loader = PyPDFLoader(file_path)
                documents.extend(loader.load())
    else:
        print(f"è·¯å¾„ '{directory_path}' æ—¢ä¸æ˜¯PDFæ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•")
        
    return documents

def compute_document_hash(document) -> str:
    """
    è®¡ç®—æ–‡æ¡£å†…å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
    
    Args:
        document: æ–‡æ¡£å¯¹è±¡
        
    Returns:
        str: æ–‡æ¡£å“ˆå¸Œå€¼
    """
    content = document.page_content
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def add_documents_to_chroma(documents: List, 
                           persist_directory: str = "./chroma_db",
                           collection_name: str = "local_pdf_chunks",
                           split_type: str = "recursive") -> None:
    """
    å°†æ–‡æ¡£æ·»åŠ åˆ°Chromaå‘é‡æ•°æ®åº“
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        persist_directory: æŒä¹…åŒ–ç›®å½•
        collection_name: é›†åˆåç§°
        split_type: åˆ†å—ç±»å‹ ("recursive" æˆ– "semantic")
    """
    # è·å–æ–‡æœ¬åˆ†å‰²å™¨
    if split_type == "semantic":
        text_splitter = get_text_splitter(split_type, embeddings)
    else:
        text_splitter = get_text_splitter(split_type)
    
    # åˆ†å‰²æ–‡æ¡£
    split_documents = text_splitter.split_documents(documents)
    
    # å¢å¼ºå…ƒæ•°æ®å¹¶æ¸…æ´—æ–‡æœ¬
    for i, doc in enumerate(split_documents):
        # ç¡®ä¿docæ˜¯æ–‡æ¡£å¯¹è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²
        if isinstance(doc, str):
            print(f"è­¦å‘Š: å‘ç°å­—ç¬¦ä¸²ç±»å‹çš„æ–‡æ¡£å—ï¼Œè·³è¿‡å¤„ç†: {doc[:50]}...")
            continue
            
        # ç¡®ä¿æœ‰å…ƒæ•°æ®å­—å…¸
        if doc.metadata is None:
            doc.metadata = {}
            
        # æ·»åŠ åˆ†å—ç›¸å…³ä¿¡æ¯
        doc.metadata["chunk_index"] = i
        doc.metadata["total_chunks"] = len(split_documents)
        
        # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
        doc.metadata["processed_at"] = datetime.datetime.now().isoformat()
        
        # æ¸…æ´—æ–‡æœ¬å†…å®¹
        doc.page_content = clean_text(doc.page_content)
        
        # ä¸ºæ–‡æ¡£å¯¹è±¡æ·»åŠ IDå±æ€§ï¼ˆChromaéœ€è¦ï¼‰
        if not hasattr(doc, 'id'):
            # ä½¿ç”¨æºæ–‡ä»¶ä¿¡æ¯å’Œç´¢å¼•ç”Ÿæˆå”¯ä¸€ID
            source = doc.metadata.get("source", "unknown")
            doc.id = f"{source}_chunk_{i}"
    
    # åˆ›å»ºæˆ–åŠ è½½Chromaæ•°æ®åº“
    if os.path.exists(persist_directory):
        # åŠ è½½ç°æœ‰æ•°æ®åº“
        db = Chroma(
            collection_name=collection_name,
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
        
        # è·å–ç°æœ‰æ–‡æ¡£çš„IDså’Œå…ƒæ•°æ®
        existing_docs = db.get(include=["documents", "metadatas"])
        existing_contents = existing_docs["documents"]
        
        # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ–‡æ¡£
        new_documents = []
        for i, doc in enumerate(split_documents):
            # ç¡®ä¿docæ˜¯æ–‡æ¡£å¯¹è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²
            if isinstance(doc, str):
                continue
                
            if doc.page_content not in existing_contents:
                new_documents.append(doc)
                
        if new_documents:
            db.add_documents(new_documents)
            print(f"æ·»åŠ äº† {len(new_documents)} ä¸ªæ–°æ–‡æ¡£å—åˆ°æ•°æ®åº“")
        else:
            print("æ²¡æœ‰æ–°çš„æ–‡æ¡£å—éœ€è¦æ·»åŠ ")
    else:
        # è¿‡æ»¤æ‰å­—ç¬¦ä¸²ç±»å‹çš„æ–‡æ¡£
        filtered_documents = [doc for doc in split_documents if not isinstance(doc, str)]
        
        # åˆ›å»ºæ–°çš„æ•°æ®åº“
        db = Chroma.from_documents(
            filtered_documents,
            embeddings,
            collection_name=collection_name,
            persist_directory=persist_directory
        )
        print(f"åˆ›å»ºäº†æ–°çš„Chromaæ•°æ®åº“ï¼ŒåŒ…å« {len(filtered_documents)} ä¸ªæ–‡æ¡£å—")

def show_database_info(persist_directory: str = "./RAG/tools/chroma_db",
                      collection_name: str = "local_pdf_chunks"):
    """
    å±•ç¤ºæ•°æ®åº“ç»“æ„å’Œéƒ¨åˆ†æ–‡æœ¬å—ä¿¡æ¯
    
    Args:
        persist_directory: æŒä¹…åŒ–ç›®å½•
        collection_name: é›†åˆåç§°
    """
    if not os.path.exists(persist_directory):
        print(f"æ•°æ®åº“ç›®å½• {persist_directory} ä¸å­˜åœ¨")
        return
    
    # åŠ è½½æ•°æ®åº“
    db = Chroma(
        collection_name=collection_name,
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    # è·å–æ•°æ®åº“ä¿¡æ¯
    docs = db.get(include=["documents", "metadatas"])
    
    print(f"\n=== æ•°æ®åº“ä¿¡æ¯ ===")
    print(f"é›†åˆåç§°: {collection_name}")
    print(f"å­˜å‚¨è·¯å¾„: {persist_directory}")
    print(f"æ–‡æ¡£æ€»æ•°: {len(docs['ids'])}")
    
    if len(docs['ids']) > 0:
        print(f"\n=== æ–‡æ¡£ç¤ºä¾‹ (æ˜¾ç¤ºå‰3ä¸ª) ===")
        for i in range(min(3, len(docs['ids']))):
            print(f"\n--- æ–‡æ¡£ {i+1} ---")
            print(f"ID: {docs['ids'][i]}")
            print(f"å†…å®¹é¢„è§ˆ: {docs['documents'][i][:200]}...")
            print(f"å…ƒæ•°æ®: {docs['metadatas'][i]}")
    else:
        print("\næ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£")

def show_all_collections(persist_directory: str = "./RAG/tools/chroma_db"):
    """
    æ˜¾ç¤ºæ‰€æœ‰é›†åˆåç§°
    
    Args:
        persist_directory: æŒä¹…åŒ–ç›®å½•
    """
    if not os.path.exists(persist_directory):
        print(f"æ•°æ®åº“ç›®å½• {persist_directory} ä¸å­˜åœ¨")
        return
    
    # åŠ è½½æ•°æ®åº“
    db = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    # è·å–æ‰€æœ‰é›†åˆ
    try:
        # æ³¨æ„ï¼šChromaçš„æ–°ç‰ˆæœ¬å¯èƒ½æ²¡æœ‰list_collectionsæ–¹æ³•
        collections = db._client.list_collections()
        print(f"\n=== æ‰€æœ‰é›†åˆ ===")
        for collection in collections:
            print(f"- {collection.name}")
    except AttributeError:
        print("å½“å‰Chromaç‰ˆæœ¬ä¸æ”¯æŒåˆ—å‡ºæ‰€æœ‰é›†åˆ")

def clear_collection(persist_directory: str = "./chroma_db",
                    collection_name: str = "local_pdf_chunks"):
    """
    æ¸…ç©ºæŒ‡å®šçš„Chromaé›†åˆï¼ˆä¿ç•™é›†åˆç»“æ„ï¼‰
    
    Args:
        persist_directory: æŒä¹…åŒ–ç›®å½•
        collection_name: é›†åˆåç§°
    """
    if not os.path.exists(persist_directory):
        print(f"æ•°æ®åº“ç›®å½• {persist_directory} ä¸å­˜åœ¨")
        return False
    
    try:
        # åŠ è½½æ•°æ®åº“
        db = Chroma(
            collection_name=collection_name,
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
        
        # è·å–å½“å‰æ–‡æ¡£æ•°é‡
        docs = db.get(include=["documents"])
        doc_count = len(docs["ids"])
        
        if doc_count > 0:
            # åˆ é™¤æ‰€æœ‰æ–‡æ¡£
            db.delete(ids=docs["ids"])
            print(f"å·²ä»é›†åˆ {collection_name} ä¸­æ¸…ç©º {doc_count} ä¸ªæ–‡æ¡£")
        else:
            print(f"é›†åˆ {collection_name} ä¸­æ²¡æœ‰æ–‡æ¡£éœ€è¦æ¸…ç©º")
        
        return True
        
    except Exception as e:
        print(f"æ¸…ç©ºé›†åˆæ—¶å‡ºé”™: {e}")
        return False

def delete_collection(persist_directory: str = "./chroma_db",
                     collection_name: str = "local_pdf_chunks"):
    """
    åˆ é™¤æŒ‡å®šçš„Chromaé›†åˆï¼ˆåŒ…æ‹¬é›†åˆç»“æ„å’Œæ‰€æœ‰æ•°æ®ï¼‰
    
    Args:
        persist_directory: æŒä¹…åŒ–ç›®å½•
        collection_name: é›†åˆåç§°
    """
    if not os.path.exists(persist_directory):
        print(f"æ•°æ®åº“ç›®å½• {persist_directory} ä¸å­˜åœ¨")
        return False
    
    try:
        # åŠ è½½æ•°æ®åº“
        db = Chroma(
            collection_name=collection_name,
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
        
        # åˆ é™¤é›†åˆ
        db._client.delete_collection(collection_name)
        print(f"å·²åˆ é™¤é›†åˆ: {collection_name}")
        
        return True
        
    except Exception as e:
        print(f"åˆ é™¤é›†åˆæ—¶å‡ºé”™: {e}")
        return False

def process_pdfs(pdf_directory: str = "RAG/Dataset/PDF",
                 chunk_size: int = 350,
                 chunk_overlap: int = 40,
                 persist_directory: str = "./RAG/tools/chroma_db",
                 collection_name: str = "local_pdf_chunks",
                 split_type: str = "recursive"):
    """
    å¤„ç†PDFæ–‡ä»¶å¹¶å°†å…¶åˆ†å—å­˜å‚¨åˆ°Chromaå‘é‡æ•°æ®åº“
    
    Args:
        pdf_directory: PDFæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
        chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
        chunk_overlap: æ–‡æœ¬é‡å å¤§å°
        collection_name: Chromaé›†åˆåç§°
    """
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(pdf_directory):
        print(f"è·¯å¾„ '{pdf_directory}' ä¸å­˜åœ¨")
        return
    
    print(f"å¼€å§‹å¤„ç†PDF: {pdf_directory}")
    
    # åŠ è½½PDFæ–‡æ¡£
    documents = load_pdfs_from_directory(pdf_directory)
    print(f"åŠ è½½äº† {len(documents)} ä¸ªPDFæ–‡æ¡£")
    
    if not documents:
        print("æœªæ‰¾åˆ°ä»»ä½•PDFæ–‡æ¡£")
        return
    
    # æ·»åŠ æ–‡æ¡£åˆ°Chromaæ•°æ®åº“
    add_documents_to_chroma(documents, persist_directory, collection_name, split_type)
    
    print("PDFå¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    # é»˜è®¤ä½¿ç”¨é€’å½’åˆ†å—å¤„ç†PDF
    process_pdfs(pdf_directory="RAG/Dataset/PDF/åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äººæŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶_é—«è…¾.pdf", 
                 chunk_size=350, 
                 chunk_overlap=40, 
                 split_type="semantic"
                 )

    # show_all_collections()

    # show_database_info()
    
    # clear_collection("RAG/tools/chroma_db", "local_pdf_chunks")