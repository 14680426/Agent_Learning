from typing import List
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from RAG.query.base import BaseRAG


class AbstractQueryRAG(BaseRAG):
    """æŠ½è±¡åŒ–æŸ¥è¯¢RAGå®ç°ï¼ˆStep Back RAGï¼‰"""
    
    def __init__(self, vectorstore, llm):
        """
        åˆå§‹åŒ–æŠ½è±¡åŒ–æŸ¥è¯¢RAG
        
        Args:
            vectorstore: å‘é‡æ•°æ®åº“
            llm: è¯­è¨€æ¨¡å‹ï¼ˆç”¨äºç”ŸæˆæŠ½è±¡é—®é¢˜å’Œæœ€ç»ˆç­”æ¡ˆï¼‰
        """
        super().__init__(vectorstore, llm)
        self.retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        print("âœ… æŠ½è±¡åŒ–æŸ¥è¯¢RAGåˆå§‹åŒ–å®Œæˆ")
    
    def retrieve(self, question: str, k: int = 3) -> List[Document]:
        """æ‰§è¡ŒæŠ½è±¡åŒ–æŸ¥è¯¢RAGæ£€ç´¢"""
        print(f"â“ åŸå§‹é—®é¢˜: {question}")
        print("-" * 50)
        
        # Step 1: ç”ŸæˆæŠ½è±¡åŒ–é—®é¢˜
        abstract_question = self._generate_abstract_question(question)
        print(f"ğŸ“š æŠ½è±¡åŒ–é—®é¢˜: {abstract_question}")
        
        # Step 2: æ£€ç´¢èƒŒæ™¯çŸ¥è¯†ï¼ˆæŠ½è±¡é—®é¢˜ï¼‰
        background_docs = self._retrieve_documents(abstract_question, k)
        print(f"ğŸ” æ£€ç´¢åˆ° {len(background_docs)} ä¸ªèƒŒæ™¯çŸ¥è¯†æ–‡æ¡£")
        
        # Step 3: æ£€ç´¢å…·ä½“ä¿¡æ¯ï¼ˆåŸå§‹é—®é¢˜ï¼‰
        specific_docs = self._retrieve_documents(question, k)
        print(f"ğŸ“„ æ£€ç´¢åˆ° {len(specific_docs)} ä¸ªå…·ä½“ä¿¡æ¯æ–‡æ¡£")
        
        # Step 4: åˆå¹¶å¹¶å»é‡æ–‡æ¡£
        all_docs = background_docs + specific_docs
        unique_docs = self._get_unique_documents([all_docs])
        print(f"âœ¨ åˆå¹¶å»é‡åå‰©ä½™ {len(unique_docs)} ä¸ªæ–‡æ¡£")
        
        return unique_docs
    
    def _generate_abstract_question(self, question: str) -> str:
        """ç”ŸæˆæŠ½è±¡åŒ–é—®é¢˜"""
        # å®šä¹‰æç¤ºæ¨¡æ¿
        prompt_template = PromptTemplate.from_template(
            "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿å°†å…·ä½“é—®é¢˜è½¬åŒ–ä¸ºæ›´æŠ½è±¡ã€æ›´æ¦‚æ‹¬çš„é—®é¢˜ã€‚\n\n"
            "ç»™å®šä¸€ä¸ªå…·ä½“é—®é¢˜ï¼Œè¯·ç”Ÿæˆä¸€ä¸ªèƒ½å¤Ÿæä¾›èƒŒæ™¯çŸ¥è¯†çš„æ›´æŠ½è±¡é—®é¢˜ã€‚è¿™ä¸ªæŠ½è±¡çš„é—®é¢˜è¦ä¸åŸæ¥çš„é—®é¢˜æœ‰ä¸€å®šçš„å…³è”æ€§ï¼Œä¸èƒ½å¤Ÿè™šæ„ã€‚\n\n"
            "ç¤ºä¾‹ï¼š\n"
            "å…·ä½“é—®é¢˜: \"ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ\"\n"
            "Step Backé—®é¢˜: \"é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†å¹¶ä¸”æœ‰ä»€ä¹ˆåº”ç”¨ï¼Ÿ\"\n\n"
            "å…·ä½“é—®é¢˜: \"è¯·æ£€ç´¢é‡å­è®¡ç®—çš„ç›¸å…³ä¿¡æ¯ã€‚\"\n"
            "Step Backé—®é¢˜: \"è¯·ç”¨å­¦æœ¯è¯­è¨€æè¿°é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†ã€‚\"\n\n"
            "ç°åœ¨è¯·ä¸ºä»¥ä¸‹å…·ä½“é—®é¢˜ç”ŸæˆStep Backé—®é¢˜ï¼š\n\n"
            "å…·ä½“é—®é¢˜: {question}\n"
            "Step Backé—®é¢˜:"
        )
        
        # æ ¼å¼åŒ–æç¤º
        prompt = prompt_template.format(question=question)
        
        # ä½¿ç”¨LLMç”ŸæˆæŠ½è±¡é—®é¢˜
        try:
            response = self.llm.invoke(prompt)
            abstract_question = response.content.strip()
        except Exception as e:
            print(f"âš ï¸  LLMè°ƒç”¨å‡ºé”™: {e}")
            # è¿”å›åŸå§‹é—®é¢˜ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            return question
        
        return abstract_question
    
    def _retrieve_documents(self, query: str, k: int = 3) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs[:k]
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥ '{query}': {e}")
            return []
    
    def _get_unique_documents(self, documents: List[List[Document]]) -> List[Document]:
        """å»é‡æ–‡æ¡£"""
        unique_docs = {}
        for doc_list in documents:
            for doc in doc_list:
                content = doc.page_content
                if content not in unique_docs:
                    unique_docs[content] = doc
        return list(unique_docs.values())


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç  - å±•ç¤ºå¦‚ä½•ä½¿ç”¨AbstractQueryRAG
    print("AbstractQueryRAGæ£€ç´¢æµ‹è¯•")
    print("=" * 50)
    
    import os
    # åˆ›å»ºæœ¬åœ°æ¨¡å‹ç›®å½•
    local_models_dir = "Models"
    os.makedirs(local_models_dir, exist_ok=True)

    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æ¨¡å‹ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ä» ModelScope ä¸‹è½½
    from modelscope.hub.snapshot_download import snapshot_download
    model_id = "maidalun/bce-embedding-base_v1"
    
    # æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„
    local_model_path = os.path.join(local_models_dir, "maidalun", "bce-embedding-base_v1")
    
    # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨æˆ–è€…ç›®å½•ä¸ºç©ºï¼Œåˆ™ä» ModelScope ä¸‹è½½
    if not os.path.exists(local_model_path) or not os.listdir(local_model_path):
        print(f"ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ {model_id}ï¼Œæ­£åœ¨ä» ModelScope ä¸‹è½½...")
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(local_model_path), exist_ok=True)
        local_model_path = snapshot_download(model_id, local_dir=local_model_path)
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_path}")
    else:
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")

    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¯ç”¨ GPUï¼‰
    from langchain_community.embeddings import HuggingFaceEmbeddings
    embeddings = HuggingFaceEmbeddings(
        model_name=local_model_path,
        model_kwargs={"device": "cuda"},               # ä½¿ç”¨ GPU åŠ é€Ÿ
        encode_kwargs={"normalize_embeddings": True}   # å½’ä¸€åŒ–ä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    )

    # åŠ è½½Chromaå‘é‡æ•°æ®åº“ï¼ŒæŒ‡å®šé›†åˆåç§°
    from langchain_chroma import Chroma
    vectorstore = Chroma(
        persist_directory="RAG/tools/chroma_db",
        embedding_function=embeddings,
        collection_name="local_pdf_chunks"  # æŒ‡å®šé»˜è®¤é›†åˆåç§°
    )
    
    # åˆ›å»ºAbstractQueryRAGå¯¹è±¡
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    rag = AbstractQueryRAG(vectorstore, llm)
    
    # æ‰§è¡ŒæŸ¥è¯¢
    question = "ä»€ä¹ˆæ˜¯æ»‘åŠ¨æ£€æµ‹ï¼Ÿ"
    print(f"\nğŸ” æŸ¥è¯¢é—®é¢˜: {question}")
    docs = rag.retrieve(question)
    print(f"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
    
    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
    for i, doc in enumerate(docs):
        print(f"\n[{i+1}] {doc.page_content[:200]}...")