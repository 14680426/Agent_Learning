from typing import List
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from RAG.query.base import BaseRAG


class MultiQueryRAG(BaseRAG):
    """Multi-Query RAGæ£€ç´¢å®ç°"""
    
    def __init__(self, vectorstore, llm):
        """
        åˆå§‹åŒ–Multi-Query RAGæ£€ç´¢
        
        Args:
            vectorstore: å‘é‡æ•°æ®åº“
            llm: è¯­è¨€æ¨¡å‹ï¼ˆç”¨äºç”ŸæˆæŸ¥è¯¢å˜ä½“ï¼‰
        """
        super().__init__(vectorstore, llm)
        self.llm = llm
        self.retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        print("âœ… Multi-Query RAGæ£€ç´¢åˆå§‹åŒ–å®Œæˆ")
    
    def retrieve(self, question: str, k: int = 3) -> List[Document]:
        """æ‰§è¡ŒMulti-Query RAGæ£€ç´¢"""
        print(f"ğŸ¯ åŸå§‹é—®é¢˜: {question}")
        print("-" * 50)
        
        # Step 1: ç”ŸæˆæŸ¥è¯¢å˜ä½“
        queries = self._generate_query_variants(question)
        print(f"ğŸ“ ç”Ÿæˆäº† {len(queries)} ä¸ªæŸ¥è¯¢:")
        for i, q in enumerate(queries, 1):
            print(f"   {i}. {q}")
        
        # Step 2: æ£€ç´¢æ–‡æ¡£
        all_docs = self._retrieve_documents_multi(queries, k)
        total_docs_before_dedup = sum(len(docs) for docs in all_docs)
        print(f"ğŸ“š æ£€ç´¢åˆ° {total_docs_before_dedup} ä¸ªæ–‡æ¡£ï¼ˆå«é‡å¤ï¼‰")
        
        # Step 3: å»é‡
        unique_docs = self._get_unique_documents(all_docs)
        print(f"âœ¨ å»é‡åå‰©ä½™ {len(unique_docs)} ä¸ªæ–‡æ¡£")
        
        return unique_docs
    
    def _generate_query_variants(self, question: str) -> List[str]:
        """ç”ŸæˆæŸ¥è¯¢å˜ä½“"""
        # å®šä¹‰æç¤ºæ¨¡æ¿
        query_prompt_template = PromptTemplate.from_template(
            "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿å°†ç”¨æˆ·çš„é—®é¢˜æ”¹å†™æˆå¤šä¸ªè¯­ä¹‰ç›¸åŒä½†è¡¨è¾¾ä¸åŒçš„æœç´¢æŸ¥è¯¢ã€‚æ¯ä¸ªæŸ¥è¯¢åº”ç®€æ´ã€ç‹¬ç«‹ï¼Œé€‚åˆç”¨äºå‘é‡æ£€ç´¢ã€‚\n\n"
            "åŸå§‹é—®é¢˜ï¼š{question}\n\n"
            "è¯·ç”Ÿæˆ3ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è§£é‡Šï¼š"
        )
        
        # æ ¼å¼åŒ–æç¤º
        query_prompt = query_prompt_template.format(question=question)
        
        # ä½¿ç”¨ChatOpenAIç”ŸæˆæŸ¥è¯¢å˜ä½“ï¼Œä¿®å¤å‚æ•°é—®é¢˜
        try:
            response = self.llm.invoke(query_prompt)
            response_text = response.content.strip()
        except Exception as e:
            print(f"âš ï¸  LLMè°ƒç”¨å‡ºé”™: {e}")
            # è¿”å›åŸå§‹é—®é¢˜ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            return [question]
        
        # è§£æç”Ÿæˆçš„æŸ¥è¯¢
        queries = []
        lines = response_text.split('\n')
        for line in lines:
            line = line.strip()
            if line and len(line) > 5:
                # ç§»é™¤å¯èƒ½çš„ç¼–å·æ ‡è®°
                if line[0].isdigit() and (line[1] == '.' or line[1] == 'ã€'):
                    query = line[2:].strip()
                else:
                    query = line
                queries.append(query)
        
        # ç¡®ä¿åŒ…å«åŸå§‹é—®é¢˜å¹¶å»é‡
        all_queries = [question] + queries
        unique_queries = []
        seen = set()
        for q in all_queries:
            if q not in seen:
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries[:4]  # æœ€å¤šè¿”å›4ä¸ªæŸ¥è¯¢
    
    def _retrieve_documents_multi(self, queries: List[str], k: int = 3) -> List[List[Document]]:
        """æ£€ç´¢æ‰€æœ‰æŸ¥è¯¢çš„æ–‡æ¡£"""
        all_docs = []
        for query in queries:
            try:
                docs = self.vectorstore.similarity_search(query, k=k)
                all_docs.append(docs)
                print(f"   ğŸ” '{query}': æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                print(f"âŒ æ£€ç´¢å¤±è´¥ '{query}': {e}")
                all_docs.append([])
        return all_docs
    
    def _get_unique_documents(self, documents: List[List[Document]]) -> List[Document]:
        """å»é‡æ–‡æ¡£"""
        unique_docs = {}
        for doc_list in documents:
            for doc in doc_list:
                content = doc.page_content
                if content not in unique_docs:
                    unique_docs[content] = doc
        return list(unique_docs.values())


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç  - å±•ç¤ºå¦‚ä½•ä½¿ç”¨MultiQueryRAG
    print("MultiQueryRAGæ£€ç´¢æµ‹è¯•")
    print("=" * 50)
    
    import os
    # åˆ›å»ºæœ¬åœ°æ¨¡å‹ç›®å½•
    local_models_dir = "Models"
    os.makedirs(local_models_dir, exist_ok=True)

    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æ¨¡å‹ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ä» ModelScope ä¸‹è½½
    from modelscope.hub.snapshot_download import snapshot_download
    model_id = "maidalun/bce-embedding-base_v1"
    
    # æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„
    local_model_path = os.path.join(local_models_dir, "maidalun", "bce-embedding-base_v1")
    
    # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨æˆ–è€…ç›®å½•ä¸ºç©ºï¼Œåˆ™ä» ModelScope ä¸‹è½½
    if not os.path.exists(local_model_path) or not os.listdir(local_model_path):
        print(f"ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ {model_id}ï¼Œæ­£åœ¨ä» ModelScope ä¸‹è½½...")
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(local_model_path), exist_ok=True)
        local_model_path = snapshot_download(model_id, local_dir=local_model_path)
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_path}")
    else:
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")

    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¯ç”¨ GPUï¼‰
    from langchain_community.embeddings import HuggingFaceEmbeddings
    embeddings = HuggingFaceEmbeddings(
        model_name=local_model_path,
        model_kwargs={"device": "cuda"},               # ä½¿ç”¨ GPU åŠ é€Ÿ
        encode_kwargs={"normalize_embeddings": True}   # å½’ä¸€åŒ–ä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    )

    # åŠ è½½Chromaå‘é‡æ•°æ®åº“ï¼ŒæŒ‡å®šé›†åˆåç§°
    from langchain_community.vectorstores import Chroma
    vectorstore = Chroma(
        persist_directory="RAG/tools/chroma_db",
        embedding_function=embeddings,
        collection_name="local_pdf_chunks"  # æŒ‡å®šé»˜è®¤é›†åˆåç§°
    )
    
    # åˆ›å»ºMultiQueryRAGå¯¹è±¡
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    rag = MultiQueryRAG(vectorstore, llm)
    
    # æ‰§è¡ŒæŸ¥è¯¢
    question = "ä»€ä¹ˆæ˜¯æ»‘åŠ¨æ£€æµ‹ï¼Ÿ"
    print(f"\nğŸ” æŸ¥è¯¢é—®é¢˜: {question}")
    docs = rag.retrieve(question)
    print(f"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
    
    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
    for i, doc in enumerate(docs):
        print(f"\n[{i+1}] {doc.page_content[:200]}...")
