from typing import List
from langchain_core.documents import Document
from RAG.query.base import BaseRAG


class SingleQueryRAG(BaseRAG):
    """Single-Query RAGæ£€ç´¢å®ç°"""
    
    def __init__(self, vectorstore, llm=None):
        """
        åˆå§‹åŒ–Single-Query RAGæ£€ç´¢
        
        Args:
            vectorstore: å‘é‡æ•°æ®åº“
            llm: è¯­è¨€æ¨¡å‹
        """
        super().__init__(vectorstore, llm)
        print("âœ… Single-Query RAGæ£€ç´¢åˆå§‹åŒ–å®Œæˆ")
    
    def retrieve(self, question: str, k: int = 3) -> List[Document]:
        """æ‰§è¡ŒSingle-Query RAGæ£€ç´¢"""
        print(f"ğŸ¯ é—®é¢˜: {question}")
        print("-" * 50)
        
        # ç›´æ¥æ£€ç´¢æ–‡æ¡£
        try:
            docs = self.vectorstore.similarity_search(question, k=k)
            print(f"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            docs = []
        
        return docs


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç  - å±•ç¤ºå¦‚ä½•ä½¿ç”¨SingleQueryRAG
    print("SingleQueryRAGæ£€ç´¢æµ‹è¯•")
    print("=" * 50)
    
    import os
    # åˆ›å»ºæœ¬åœ°æ¨¡å‹ç›®å½•
    local_models_dir = "Models"
    os.makedirs(local_models_dir, exist_ok=True)

    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æ¨¡å‹ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ä» ModelScope ä¸‹è½½
    from modelscope.hub.snapshot_download import snapshot_download
    model_id = "maidalun/bce-embedding-base_v1"
    
    # æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„
    local_model_path = os.path.join(local_models_dir, "maidalun", "bce-embedding-base_v1")
    
    # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨æˆ–è€…ç›®å½•ä¸ºç©ºï¼Œåˆ™ä» ModelScope ä¸‹è½½
    if not os.path.exists(local_model_path) or not os.listdir(local_model_path):
        print(f"ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ {model_id}ï¼Œæ­£åœ¨ä» ModelScope ä¸‹è½½...")
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(local_model_path), exist_ok=True)
        local_model_path = snapshot_download(model_id, local_dir=local_model_path)
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_path}")
    else:
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")

    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¯ç”¨ GPUï¼‰
    from langchain_community.embeddings import HuggingFaceEmbeddings
    embeddings = HuggingFaceEmbeddings(
        model_name=local_model_path,
        model_kwargs={"device": "cuda"},               # ä½¿ç”¨ GPU åŠ é€Ÿ
        encode_kwargs={"normalize_embeddings": True}   # å½’ä¸€åŒ–ä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    )

    # åŠ è½½Chromaå‘é‡æ•°æ®åº“ï¼ŒæŒ‡å®šé›†åˆåç§°
    from langchain_community.vectorstores import Chroma
    vectorstore = Chroma(
        persist_directory="RAG/tools/chroma_db",
        embedding_function=embeddings,
        collection_name="local_pdf_chunks"  # æŒ‡å®šé»˜è®¤é›†åˆåç§°
    )
    
    # åˆ›å»ºSingleQueryRAGå¯¹è±¡
    rag = SingleQueryRAG(vectorstore)
    
    # æ‰§è¡ŒæŸ¥è¯¢
    question = "ä»€ä¹ˆæ˜¯æ»‘åŠ¨æ£€æµ‹ï¼Ÿ"
    print(f"\nğŸ” æŸ¥è¯¢é—®é¢˜: {question}")
    docs = rag.retrieve(question)
    print(f"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
    
    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
    for i, doc in enumerate(docs):
        print(f"\n[{i+1}] {doc.page_content[:200]}...")

    # å®é™…ä½¿ç”¨ç¤ºä¾‹:
    # rag = SingleQueryRAG(vectorstore)
    # docs = rag.retrieve("ä½ çš„é—®é¢˜")
    # for i, doc in enumerate(docs):
    #     print(f"æ–‡æ¡£ {i+1}: {doc.page_content[:100]}...")
    #     print(f"å…ƒæ•°æ®: {doc.metadata}")
    #     print("-" * 50)