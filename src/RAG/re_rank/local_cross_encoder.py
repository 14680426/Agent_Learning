from sentence_transformers import CrossEncoder
from typing import List
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
import chromadb
from chromadb.config import Settings
from RAG.re_rank.base import BaseReranker
import os


class LocalCrossEncoderReranker(BaseReranker):
    """ä½¿ç”¨æœ¬åœ°äº¤å‰ç¼–ç å™¨æ¨¡å‹é‡æ’åº"""
    
    def __init__(self, embeddings, model_path: str = None):
        self.embeddings = embeddings
        
        if model_path is None:
            raise ValueError("å¿…é¡»æä¾›äº¤å‰ç¼–ç å™¨æ¨¡å‹è·¯å¾„ï¼Œæ— æ³•ä½¿ç”¨é»˜è®¤è·¯å¾„ã€‚è¯·æŒ‡å®šmodel_pathå‚æ•°ã€‚")
        
        print(f"ğŸ” ä½¿ç”¨äº¤å‰ç¼–ç å™¨æ¨¡å‹è·¯å¾„: {model_path}")
        
        # ç›´æ¥åŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹: {model_path}")
        self.cross_encoder = CrossEncoder(model_path)
        
        # è®¾ç½®æŒä¹…åŒ–ç›®å½•
        src_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.persist_directory: str = os.path.join(src_dir, "RAG/tools/chroma_db")

        client = chromadb.PersistentClient(path=self.persist_directory)  
        print("ğŸ” ç›´æ¥è¿æ¥åˆ°ç°æœ‰çš„ Chroma é›†åˆ 'local_rerank'")
        
        # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
        self.vectorstore = Chroma(
            collection_name="local_rerank",
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings
        )
        print("âœ… äº¤å‰ç¼–ç å™¨æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def rerank(self, query: str, docs: List[Document], k: int = 5) -> List[Document]:
        """
        ä½¿ç”¨äº¤å‰ç¼–ç å™¨å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº
        
        Args:
            query: æŸ¥è¯¢è¯­å¥
            docs: å¾…æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
        query_doc_pairs = [
            [query, doc.page_content] for doc in docs
        ]
        
        # ä½¿ç”¨äº¤å‰ç¼–ç å™¨è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        print(f"ğŸ¯ ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ–°è¯„åˆ†...")
        scores = self.cross_encoder.predict(query_doc_pairs)
        
        # æ ¹æ®åˆ†æ•°æ’åº
        scored_docs = [
            {'document': doc, 'score': score}
            for doc, score in zip(docs, scores)
        ]
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        # è¿”å›top-k
        reranked_docs = scored_docs[:k]
        
        print("\né‡æ’åºç»“æœ:")
        for i, item in enumerate(reranked_docs):
            print(f"{i+1}. [å¾—åˆ†: {item['score']:.4f}] {item['document'].page_content[:100]}...")
        
        return [item['document'] for item in reranked_docs]

    def retrieve_and_rerank(
        self,
        query: str,
        initial_k: int = 20,
        final_k: int = 5
    ):
        """æ£€ç´¢å¹¶é‡æ’åº"""
        # 1. åˆå§‹æ£€ç´¢
        initial_docs = self.vectorstore.similarity_search(query, k=initial_k)
        
        # 2. è°ƒç”¨rerankæ–¹æ³•è¿›è¡Œé‡æ’åº
        final_docs = self.rerank(query, initial_docs, final_k)
        
        return final_docs
    
    def load_vectorstore(self, embedding_model_path: str = None):
        """åŠ è½½å‘é‡å­˜å‚¨"""
        print("ğŸ“‚ åŠ è½½å‘é‡å­˜å‚¨...")
        
        # å¦‚æœæ²¡æœ‰æä¾›åµŒå…¥æ¨¡å‹è·¯å¾„ï¼Œåˆ™æç¤ºç”¨æˆ·å¿…é¡»æä¾›è·¯å¾„
        if embedding_model_path is None:
            raise ValueError("å¿…é¡»æä¾›åµŒå…¥æ¨¡å‹è·¯å¾„ï¼Œæ— æ³•ä½¿ç”¨é»˜è®¤è·¯å¾„ã€‚è¯·æŒ‡å®šembedding_model_pathå‚æ•°ã€‚")
        
        print(f"âœ… ä½¿ç”¨åµŒå…¥æ¨¡å‹: {embedding_model_path}")
            
        from langchain_community.embeddings import HuggingFaceEmbeddings
        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path)
        # è·å–srcç›®å½•è·¯å¾„
        src_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.vectorstore = Chroma(
            persist_directory=os.path.join(src_dir, "RAG/tools/chroma_db"),
            embedding_function=embeddings,
            collection_name="local_pdf_chunks"
        )
        print("âœ… å‘é‡å­˜å‚¨åŠ è½½å®Œæˆ")

def main():
    """æµ‹è¯•æœ¬åœ°äº¤å‰ç¼–ç å™¨é‡æ’åºåŠŸèƒ½"""
    # ç”±äºä¾èµ–ç‰¹å®šçš„åµŒå…¥æ¨¡å‹å’Œç¯å¢ƒé…ç½®ï¼Œè¿™é‡Œåªå±•ç¤ºæµ‹è¯•ç»“æ„
    # å®é™…æµ‹è¯•éœ€è¦æ ¹æ®å…·ä½“ç¯å¢ƒé…ç½®è¿›è¡Œ
    
    print("=" * 50)
    print("æœ¬åœ°äº¤å‰ç¼–ç å™¨é‡æ’åºæµ‹è¯•")
    print("=" * 50)
    
    
    from langchain_core.documents import Document
    from langchain_community.embeddings import HuggingFaceEmbeddings
    
    # è·å–srcç›®å½•è·¯å¾„
    src_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    model_path = os.path.join(src_dir, "Models", "maidalun", "bce-embedding-base_v1")
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(model_name=model_path)
    
    # åˆ›å»ºé‡æ’åºå™¨å®ä¾‹ï¼Œéœ€è¦æä¾›æ¨¡å‹è·¯å¾„
    reranker_model_path = os.path.join(src_dir, "Models", "ms-marco-MiniLM-L-6-v2")
    reranker = LocalCrossEncoderReranker(embeddings, reranker_model_path)
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        Document(page_content="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚", metadata={"source": "ai_intro"}),
        Document(page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºå†³ç­–æˆ–é¢„æµ‹ã€‚", metadata={"source": "ml_intro"}),
        Document(page_content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒæ¨¡ä»¿äººè„‘çš„å·¥ä½œæ–¹å¼å¤„ç†æ•°æ®å’Œåˆ›å»ºæ¨¡å¼ï¼Œç”¨äºå†³ç­–åˆ¶å®šã€‚", metadata={"source": "dl_intro"}),
        Document(page_content="è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦æ–¹å‘ï¼Œæ¶‰åŠè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚", metadata={"source": "nlp_intro"}),
        Document(page_content="è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºæ•™ä¼šè®¡ç®—æœºå¦‚ä½•è§£é‡Šå’Œç†è§£è§†è§‰ä¸–ç•Œã€‚", metadata={"source": "cv_intro"})
    ]
    
    # æ·»åŠ æ–‡æ¡£åˆ°å­˜å‚¨ä¸­
    reranker.add_documents(test_docs)
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    
    print("=" * 50)
    print("æµ‹è¯•æœ¬åœ°äº¤å‰ç¼–ç å™¨é‡æ’åº")
    print("=" * 50)
    print(f"æŸ¥è¯¢: {query}")
    print("-" * 30)
    
    # æ‰§è¡Œæ£€ç´¢å’Œé‡æ’åº
    results = reranker.retrieve_and_rerank(query, initial_k=10, final_k=3)
    
    print("\n" + "=" * 50)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 50)


if __name__ == "__main__":
    main()